= NVIDIA Container Runtime

== Install NVIDIA drivers
// === pre-installation-actions
// ----
// lspci | grep -i nvidia
// uname -m && cat /etc/*release
// gcc --version
// uname -r
// ----

=== Install
----
sudo apt-get install linux-headers-$(uname -r)
wget https://developer.download.nvidia.com/compute/cuda/repos/debian12/x86_64/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb
sudo apt-get update
sudo apt-get -y install cuda-toolkit
sudo reboot
----

// === post-installation actions
// ----
// export PATH=/usr/local/cuda-12.6/bin${PATH:+:${PATH}}
// export LD_LIBRARY_PATH=/usr/local/cuda-12.6/lib64\
//                          ${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}
// ----

- https://docs.nvidia.com/cuda/cuda-installation-guide-linux/
- https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#debian

== Install NVIDIA Container Toolkit
----
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
    sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
    sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit
----
=== test
----
# sudo ctr image pull docker.io/nvidia/cuda:12.4.1-base-ubuntu22.04
sudo ctr run --rm --gpus 0 -t nvidia/cuda:12.4.1-base-ubuntu22.04 gpu nvidia-smi
----

----
sudo systemctl restart k3s
sudo grep nvidia /var/lib/rancher/k3s/agent/etc/containerd/config.toml
----

- https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html
- https://hub.docker.com/r/nvidia

== Install NVIDIA device plugin for Kubernetes
[NOTE]
runtimeClassName: nvidia

----
helm repo add nvdp https://nvidia.github.io/k8s-device-plugin
helm repo update
helm upgrade -i nvdp nvdp/nvidia-device-plugin \
  --version 0.16.2 \
  --namespace nvidia-device-plugin \
  --create-namespace \
  --set gfd.enabled=true \
  --set runtimeClassName=nvidia \
  --set-file config.map.config=dp-mps-config.yaml

# heml uninstall nvdp -n nvidia-device-plugin
----
=== test
----
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: gpu
spec:
  restartPolicy: Never
  runtimeClassName: nvidia
  containers:
    - name: gpu
      image: "nvidia/cuda:12.4.1-base-ubuntu22.04"
      command: [ "/bin/bash", "-c", "--" ]
      args: [ "while true; do sleep 30; done;" ]
      resources:
        limits:
          nvidia.com/gpu: 1
EOF

kubectl exec -it gpu -- nvidia-smi
kubectl delete pod/gpu
----

- https://github.com/NVIDIA/k8s-device-plugin/
- https://docs.k3s.io/advanced#nvidia-container-runtime-support
- https://github.com/k3s-io/k3s/discussions/8596
- https://github.com/NVIDIA/k8s-device-plugin/issues/712


// == NVIDIA MPS
// ----
// # start
// export CUDA_VISIBLE_DEVICES=0
// export CUDA_MPS_PIPE_DIRECTORY=/tmp/nvidia-mps
// export CUDA_MPS_LOG_DIRECTORY=/tmp/nvidia-log
// nvidia-cuda-mps-control -d
//
// # usage
// export CUDA_MPS_PIPE_DIRECTORY=/tmp/nvidia-mps
// export CUDA_MPS_LOG_DIRECTORY=/tmp/nvidia-log
//
// # shutdown
// echo quit | nvidia-cuda-mps-control
// ----
// - https://docs.nvidia.com/deploy/mps/#on-a-single-user-system

:numbered!:
== References
[bibliography]
- https://itnext.io/enabling-nvidia-gpus-on-k3s-for-cuda-workloads-a11b96f967b0
- https://k3d.io/v5.7.3/usage/advanced/cuda/#the-nvidia-device-plugin
